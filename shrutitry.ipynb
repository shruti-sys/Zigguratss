{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aks\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\aks\\appdata\\local\\temp\\pip-req-build-widtqcml\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (4.66.4)\n",
      "Requirement already satisfied: torch in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (2.4.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (0.19.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\aks\\appdata\\roaming\\python\\python39\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->clip==1.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->clip==1.0) (2024.6.1)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aks\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\aks\\AppData\\Local\\Temp\\pip-req-build-widtqcml'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import psutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import clip\n",
    "\n",
    "# Function to check memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess images\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    return clip.tokenize([text]).to(device)\n",
    "\n",
    "# Paths\n",
    "images_folder = r\"C:\\Users\\aks\\Desktop\\zigguratss\\artwork\"\n",
    "memory_limit_gb = 10  # Set memory limit to 10 GB\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = r\"C:\\Users\\aks\\Desktop\\zigguratss\\output_csv_file.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     94\u001b[0m query_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgirl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 95\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mfind_most_similar_images_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m, in \u001b[0;36mfind_most_similar_images_text\u001b[1;34m(csv_path, image_folder, query_text, top_n)\u001b[0m\n\u001b[0;32m     64\u001b[0m similar_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, filenames[idx])\n\u001b[0;32m     65\u001b[0m similar_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(similar_image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimread(similar_image_path, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     67\u001b[0m orb_similarity \u001b[38;5;241m=\u001b[39m orb_sim(img, img)  \u001b[38;5;66;03m# Self-similarity\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to preprocess a single text query and return its embedding\n",
    "def get_single_text_embedding(text):\n",
    "    text_input = preprocess_text(text)\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model.encode_text(text_input)\n",
    "    text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "    return text_embedding.cpu().numpy()\n",
    "\n",
    "# Function to compute ORB similarity\n",
    "def orb_sim(img1, img2):\n",
    "    orb = cv2.ORB_create()\n",
    "    kp_a, desc_a = orb.detectAndCompute(img1, None)\n",
    "    kp_b, desc_b = orb.detectAndCompute(img2, None)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(desc_a, desc_b)\n",
    "    similar_regions = [i for i in matches if i.distance < 50]\n",
    "    if len(matches) == 0:\n",
    "        return 0\n",
    "    return len(similar_regions) / len(matches)\n",
    "\n",
    "# Function to compute SSIM\n",
    "def structural_sim(img1, img2):\n",
    "    sim, diff = structural_similarity(img1, img2, full=True)\n",
    "    return sim\n",
    "\n",
    "# Load images from a folder\n",
    "def load_image(image_folder, image_id):\n",
    "    image_path = os.path.join(image_folder, image_id)\n",
    "    return cv2.imread(image_path, 0)\n",
    "\n",
    "# Find most similar images to a text query\n",
    "def find_most_similar_images_text(csv_path, image_folder, query_text, top_n=3):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    filenames = data['image_id'].tolist()\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "        images.append(image)\n",
    "    \n",
    "    # Encode images using CLIP model\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = torch.cat([model.encode_image(image) for image in images])\n",
    "    \n",
    "    # Normalize features\n",
    "    image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    query_text_embedding = get_single_text_embedding(query_text)\n",
    "    \n",
    "    # Compute similarity scores using cosine_similarity from sklearn\n",
    "    similarity_scores = cosine_similarity(image_embeddings.cpu().numpy(), query_text_embedding).squeeze()\n",
    "    \n",
    "    # Get indices of the top N most similar images\n",
    "    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Display the most similar images and compute ORB and SSIM\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        similar_image_path = os.path.join(image_folder, filenames[idx])\n",
    "        similar_image = Image.open(similar_image_path).convert('L')\n",
    "        img = cv2.imread(similar_image_path, 0)\n",
    "        orb_similarity = orb_sim(img, img)  # Self-similarity\n",
    "        if img.shape != img.shape:\n",
    "            img_resized = resize(img, (img.shape[0], img.shape[1]), anti_aliasing=True, preserve_range=True).astype(img.dtype)\n",
    "        else:\n",
    "            img_resized = img\n",
    "        ssim = structural_sim(img, img_resized)\n",
    "\n",
    "        results.append({\n",
    "            'img_id': filenames[idx],\n",
    "            'orb_similarity': orb_similarity,\n",
    "            'ssim': ssim\n",
    "        })\n",
    "\n",
    "        plt.subplot(1, top_n, i + 1)\n",
    "        plt.imshow(similar_image)\n",
    "        plt.title(f\"Similar Image {i+1}\\nScore: {similarity_scores[idx]:.4f}\\nORB: {orb_similarity:.4f}\\nSSIM: {ssim:.4f}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define paths\n",
    "csv_path = r\"C:\\Users\\aks\\Desktop\\zigguratss\\output_csv_file.csv\"\n",
    "image_folder = r\"C:\\Users\\aks\\Desktop\\zigguratss\\artwork\"\n",
    "\n",
    "# Example usage\n",
    "query_text = 'girl'\n",
    "results = find_most_similar_images_text(csv_path, image_folder, query_text, top_n=3)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Image ID: {result['img_id']}\")\n",
    "    print(f\"ORB Similarity: {result['orb_similarity']}\")\n",
    "    print(f\"SSIM: {result['ssim']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
